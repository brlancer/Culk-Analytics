# Culk Analytics - Implementation Checklist

## Phase 1: Infrastructure Setup âœ… COMPLETE

âœ… Create project directory structure
âœ… Set up database initialization scripts
âœ… Create configuration templates (.dlt/)
âœ… Add Python extraction file skeletons (7 sources)
âœ… Write comprehensive documentation
âœ… Configure .gitignore for security
âœ… Add requirements.txt with dependencies
âœ… Create main orchestration script (run_pipeline.py)

## Phase 2: Data Extraction Implementation â³ IN PROGRESS (4 of 7 sources complete)

### Shopify (Core Commerce: B2B + DTC)
âœ… Configure dlt REST API source for Shopify
âœ… Implement incremental loading with updated_at_min
âœ… Test extraction for orders endpoint
âœ… Test extraction for products endpoint
âœ… Test extraction for customers endpoint
âœ… Handle pagination (Link headers)
âœ… Add error handling and retries
âœ… Test full pipeline: extract â†’ load â†’ verify in database
âœ… **COMPLETE** - See docs/implementation/shopify_checklist.md for details

### Faire (Wholesale)
âœ… Configure dlt REST API source for Faire
âœ… Implement dual custom header authentication (X-FAIRE-APP-CREDENTIALS + X-FAIRE-OAUTH-ACCESS-TOKEN)
âœ… Test extraction for orders endpoint
âœ… Test extraction for products endpoint
âœ… Handle cursor-based pagination
âœ… Configure write_disposition (merge for orders, replace for products)
âœ… Test full pipeline with pytest test_faire.py
âœ… **COMPLETE** - All tests passing, data loading successfully into faire_raw schema

Note: Simplified implementation using dlt REST API client with automatic nested table normalization (no custom transformers needed). Creates 5+ tables: orders, orders__items, orders__shipments, products, products__variants, etc.

### ShipHero (3PL)
âœ… Write GraphQL queries for products (inventory)
âœ… Write GraphQL queries for orders (shipments/fulfillment data)
âœ… Implement async extraction with aiohttp
âœ… Parse nested GraphQL response (edges/nodes)
âœ… Flatten data structures for dlt
âœ… Implement complexity monitoring
âœ… Test full pipeline
âœ… **COMPLETE** - See docs/implementation/shiphero_checklist.md for details

Note: OAuth token refresh flow deferred to production phase. Current implementation uses bearer token with complexity-based rate limiting and adaptive delays.

### Loop Returns
âœ… Configure dlt REST API source for Loop
âœ… Implement incremental loading
âœ… Test extraction for returns endpoint
âœ… Handle pagination (URL-based with nextPageUrl)
âœ… Implement 100-day date chunking (respects 120-day API limit)
âœ… Add PII sanitization (removes customer emails, addresses, tracking URLs)
âœ… Add error handling and retries
âœ… Test full pipeline with pytest test_loop_returns.py
âœ… **COMPLETE** - All tests passing, data loading successfully into loop_returns_raw schema

Note: Simplified implementation using custom request handling with dlt automatic nested table normalization. Creates 6 tables: returns, returns__line_items, returns__exchanges, returns__labels, returns__labels__line_items, returns__shopify_refund_object. PII sanitization removes customer emails, addresses, phone numbers, and tracking URLs before database load.

### Meta/Facebook Ads
ğŸ—“ï¸ Build API requests for Insights endpoint
ğŸ—“ï¸ Implement date range filtering
ğŸ—“ï¸ Parse Graph API responses
ğŸ—“ï¸ Extract campaign-level metrics
ğŸ—“ï¸ Handle pagination (cursor-based)
ğŸ—“ï¸ Add error handling and retries
ğŸ—“ï¸ Test full pipeline

### Google Ads
ğŸ—“ï¸ Check for dlt verified Google Ads source
ğŸ—“ï¸ Configure OAuth authentication
ğŸ—“ï¸ Build campaign performance report queries
ğŸ—“ï¸ Implement date segmentation
ğŸ—“ï¸ Extract campaign-level metrics
ğŸ—“ï¸ Monitor API quota usage
ğŸ—“ï¸ Test full pipeline

### Airtable (Product Master)
ğŸ—“ï¸ Configure dlt REST API source for Airtable
ğŸ—“ï¸ Test extraction for product table
ğŸ—“ï¸ Handle Airtable field types
ğŸ—“ï¸ Decide on full refresh vs. incremental
ğŸ—“ï¸ Test full pipeline

### Cross-Source
âœ… Update run_pipeline.py to call implemented extraction functions (Shopify, Faire, ShipHero)
âœ… Add logging for each source
âœ… Implement error handling (continue on failure)
ğŸ—“ï¸ Test orchestration of implemented sources
ğŸ—“ï¸ Verify data in PostgreSQL schemas (shopify_raw, faire_raw, shiphero_raw)
ğŸ—“ï¸ Add remaining sources when implemented (Loop, Meta Ads, Google Ads, Airtable)

## Phase 3: SQL Transformations â³ TODO

### Staging Schema
ğŸ—“ï¸ Create staging.clean_orders (deduplication, type fixes)
ğŸ—“ï¸ Create staging.clean_products (standardization)
ğŸ—“ï¸ Create staging.clean_customers
ğŸ—“ï¸ Create staging.orders_with_products (join orders + products)
ğŸ—“ï¸ Create staging.order_margins (calculate profit margins)
ğŸ—“ï¸ Create staging.inventory_snapshots (ShipHero data)
ğŸ—“ï¸ Create staging.returns_joined (Loop + order data)
ğŸ—“ï¸ Create staging.ad_spend_combined (Meta + Google Ads)

### Analytics Schema
ğŸ—“ï¸ Create analytics.dim_products (product dimension)
ğŸ—“ï¸ Create analytics.dim_customers (customer dimension)
ğŸ—“ï¸ Create analytics.dim_date (date dimension)
ğŸ—“ï¸ Create analytics.fact_orders (fact table)
ğŸ—“ï¸ Create analytics.daily_revenue (aggregation)
ğŸ—“ï¸ Create analytics.daily_ad_spend (aggregation)
ğŸ—“ï¸ Create analytics.monthly_metrics (KPIs)
ğŸ—“ï¸ Create analytics.inventory_turnover
ğŸ—“ï¸ Create analytics.return_rate_analysis
ğŸ—“ï¸ Create analytics.customer_lifetime_value

### Testing & Validation
ğŸ—“ï¸ Write SQL tests for staging transformations
ğŸ—“ï¸ Write SQL tests for analytics views
ğŸ—“ï¸ Add data quality checks (row counts, null checks)
ğŸ—“ï¸ Create sample queries for BI tools

## Phase 4: Orchestration & Monitoring â³ TODO

### Scheduling
ğŸ—“ï¸ Choose orchestration tool (Airflow / Prefect / Dagster)
ğŸ—“ï¸ Define DAG structure (dependencies between sources)
ğŸ—“ï¸ Set up scheduling (hourly for transactional, daily for ads)
ğŸ—“ï¸ Implement retry policies
ğŸ—“ï¸ Add alerting for failures

### Monitoring
ğŸ—“ï¸ Add pipeline run logging
ğŸ—“ï¸ Create dashboard for pipeline health
ğŸ—“ï¸ Monitor data freshness (last successful load)
ğŸ—“ï¸ Monitor row counts and data volume
ğŸ—“ï¸ Set up alerts for data anomalies
ğŸ—“ï¸ Track API rate limit usage

### Data Quality
ğŸ—“ï¸ Add Great Expectations or Soda checks
ğŸ—“ï¸ Validate schema consistency
ğŸ—“ï¸ Check for duplicate records
ğŸ—“ï¸ Validate foreign key relationships
ğŸ—“ï¸ Check for null values in critical fields
ğŸ—“ï¸ Monitor data distribution changes

## Phase 5: Production Readiness â³ TODO

### Infrastructure
ğŸ—“ï¸ Consider migrating to cloud warehouse (Snowflake / BigQuery)
ğŸ—“ï¸ Set up production database (separate from dev)
ğŸ—“ï¸ Implement database backups
ğŸ—“ï¸ Add read-only users for BI tools
ğŸ—“ï¸ Configure connection pooling

### Security
ğŸ—“ï¸ Move secrets to environment variables
ğŸ—“ï¸ Consider secrets manager (AWS Secrets Manager / HashiCorp Vault)
ğŸ—“ï¸ Implement database user with minimal permissions
ğŸ—“ï¸ Set up SSL for database connections
ğŸ—“ï¸ Add audit logging

### Documentation
ğŸ—“ï¸ Document SQL transformation logic
ğŸ—“ï¸ Create data dictionary (column definitions)
ğŸ—“ï¸ Write runbook for common issues
ğŸ—“ï¸ Create BI tool connection guides
ğŸ—“ï¸ Document disaster recovery procedures

### Performance
ğŸ—“ï¸ Add database indexes on foreign keys
ğŸ—“ï¸ Optimize slow SQL queries
ğŸ—“ï¸ Implement materialized views for heavy aggregations
ğŸ—“ï¸ Consider partitioning large tables by date
ğŸ—“ï¸ Monitor query performance

## Phase 6: Business Intelligence â³ TODO

### BI Tool Setup
ğŸ—“ï¸ Choose BI tool (Metabase / Looker / Tableau)
ğŸ—“ï¸ Connect BI tool to analytics schema
ğŸ—“ï¸ Create dashboards for key metrics
ğŸ—“ï¸ Build reports for stakeholders
ğŸ—“ï¸ Train team on self-service analytics

### Key Metrics to Track
ğŸ—“ï¸ Total revenue (DTC + Wholesale)
ğŸ—“ï¸ Revenue by channel
ğŸ—“ï¸ Customer acquisition cost (CAC)
ğŸ—“ï¸ Return on ad spend (ROAS)
ğŸ—“ï¸ Inventory turnover rate
ğŸ—“ï¸ Return rate
ğŸ—“ï¸ Customer lifetime value (LTV)
ğŸ—“ï¸ Average order value (AOV)

---

## Quick Commands

### Start Development
```bash
source .venv/bin/activate
python run_pipeline.py
```

### Test Individual Source
```bash
python ingestion/shopify.py
```

### Check Database
```bash
psql -U postgres -d culk_db
```

### View Logs
```bash
tail -f logs/pipeline_*.log
```

### Update Dependencies
```bash
pip install -r requirements.txt --upgrade
```

---

**Track your progress by checking off items as you complete them!** ğŸ¯
